{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"ÌòÑÏû¨ Ï†ÅÏö©Îêú Torch Î≤ÑÏ†Ñ: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from transformers import AutoImageProcessor, YolosForObjectDetection\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# --- 1. Model Loading ---\n",
    "print(\"Loading the model...\")\n",
    "# Load the processor and model from Hugging Face\n",
    "processor = AutoImageProcessor.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "model = YolosForObjectDetection.from_pretrained(\"valentinafeve/yolos-fashionpedia\")\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# --- 2. Set up data path and create image list ---\n",
    "# Set the path to your image folder\n",
    "image_folder = r\"D:/train\" \n",
    "# List of image extensions to search for\n",
    "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\"] \n",
    "image_paths = []\n",
    "# Find all images with the specified extensions in the folder\n",
    "for ext in image_extensions:\n",
    "    image_paths.extend(glob.glob(os.path.join(image_folder, ext)))\n",
    "\n",
    "if not image_paths:\n",
    "    print(f\"Warning: No images found in '{image_folder}'. Please check the path.\")\n",
    "else:\n",
    "    print(f\"Found {len(image_paths)} images to analyze.\")\n",
    "\n",
    "\n",
    "# --- 3. Helper Functions ---\n",
    "\n",
    "def get_accessory_ids(model):\n",
    "    \"\"\"Gets the IDs for accessory names from the model's configuration.\"\"\"\n",
    "    accessory_names = [\n",
    "        'hat', 'sunglasses', 'scarf', 'necktie', 'belt', 'earrings', \n",
    "        'necklace', 'watch', 'bracelet', 'bag', 'wallet', 'gloves', 'headband'\n",
    "    ]\n",
    "    label2id = model.config.label2id\n",
    "    accessory_ids = {label2id[name] for name in accessory_names if name in label2id}\n",
    "    return accessory_ids\n",
    "\n",
    "def classify_style(result, accessory_ids):\n",
    "    \"\"\"Counts accessories to classify the style.\"\"\"\n",
    "    acc_count = sum(1 for label_id in result[\"labels\"] if label_id.item() in accessory_ids)\n",
    "    return \"Maximal\" if acc_count >= 2 else \"Minimal\"\n",
    "\n",
    "def visualize_detection_and_style(image, results, style):\n",
    "    \"\"\"Draws detected objects and the final style result on the image.\"\"\"\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Display the final style result text at the top of the image\n",
    "    ax.text(20, 60, f\"Style: {style}\", fontsize=40, color='white', \n",
    "            bbox=dict(facecolor='magenta', alpha=0.8))\n",
    "\n",
    "    # Draw the detected objects\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        x, y, w, h = box[0], box[1], box[2] - box[0], box[3] - box[1]\n",
    "        \n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='cyan', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        label_text = f\"{model.config.id2label[label.item()]}: {score:.2f}\"\n",
    "        ax.text(x, y - 10, label_text, fontsize=12, color='black', bbox=dict(facecolor='cyan', alpha=0.5))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# --- 4. Main Pipeline ---\n",
    "\n",
    "# Iterate through each found image for analysis.\n",
    "for image_path in image_paths[:10]:\n",
    "    try:\n",
    "        print(f\"\\n{'='*25}\\n[Starting Analysis]: {os.path.basename(image_path)}\\n{'='*25}\")\n",
    "        \n",
    "        # Open the image\n",
    "        image_to_analyze = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # 1. Prepare the image for the YOLOS model\n",
    "        inputs = processor(images=image_to_analyze, return_tensors=\"pt\")\n",
    "        \n",
    "        # 2. Run the model (inference)\n",
    "        print(\"Running model inference...\")\n",
    "        outputs = model(**inputs)\n",
    "        print(\"Inference complete!\")\n",
    "        \n",
    "        # 3. Post-process the model output\n",
    "        target_sizes = torch.tensor([image_to_analyze.size[::-1]])\n",
    "        results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.8)[0]\n",
    "        \n",
    "        # 4. Classify the style\n",
    "        accessory_ids = get_accessory_ids(model)\n",
    "        predicted_style = classify_style(results, accessory_ids)\n",
    "        \n",
    "        # 5. Print the final results\n",
    "        detected_items = [model.config.id2label[label.item()] for label in results[\"labels\"]]\n",
    "        print(f\"\\n--- Analysis Result ---\")\n",
    "        print(f\"Detected items: {detected_items if detected_items else 'None'}\")\n",
    "        print(f\"üß• Final Style Classification: {predicted_style}\")\n",
    "        print(\"-----------------\")\n",
    "        \n",
    "        # 6. Visualize the results\n",
    "        visualize_detection_and_style(image_to_analyze, results, predicted_style)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: An error occurred while processing {os.path.basename(image_path)}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌÅ¥ÎûòÏä§ ÎùºÎ≤® Ï∂úÎ†•\n",
    "labels = model.config.id2label\n",
    "\n",
    "# Ï†ÑÏ≤¥ ÎùºÎ≤® Ï∂úÎ†•\n",
    "for idx, name in labels.items():\n",
    "    print(f\"{idx}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
